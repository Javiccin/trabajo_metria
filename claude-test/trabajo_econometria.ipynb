{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Practico de Econometria\n",
    "**Maestrias en Economia y Econometria**\n",
    "\n",
    "Seed: 1910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Seed para replicabilidad\n",
    "SEED = 1910\n",
    "\n",
    "# Parametros del modelo\n",
    "N_SIM = 5000\n",
    "BETA0_TRUE = -3.0\n",
    "BETA1_TRUE = 0.8\n",
    "OMEGA_DIAG = np.array([4.0, 9.0, 16.0, 25.0, 36.0])  # varianzas por grupo\n",
    "N_GROUPS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parte 1: Propiedades de muestra finita de FGLS (MCGE)\n",
    "\n",
    "**Modelo:** $y_i = \\beta_0 + \\beta_1 x_i + u_i$, con $\\beta_0 = -3$, $\\beta_1 = 0.8$\n",
    "\n",
    "$u \\sim N(0, \\Omega \\otimes I_N)$, $\\Omega = \\text{diag}(4, 9, 16, 25, 36)$, $x \\sim U[1, 50]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_total, beta0, beta1, rng):\n",
    "    \"\"\"\n",
    "    Genera una muestra de tamano n_total = 5*N del modelo (1).\n",
    "    Cada grupo j (j=0,...,4) tiene N = n_total/5 observaciones con varianza Omega[j,j].\n",
    "    \"\"\"\n",
    "    N = n_total // N_GROUPS\n",
    "    x = rng.uniform(1, 50, size=n_total)\n",
    "\n",
    "    # Errores con heterocedasticidad grupal\n",
    "    u = np.zeros(n_total)\n",
    "    for j in range(N_GROUPS):\n",
    "        sigma_j = np.sqrt(OMEGA_DIAG[j])\n",
    "        u[j * N:(j + 1) * N] = rng.normal(0, sigma_j, size=N)\n",
    "\n",
    "    y = beta0 + beta1 * x + u\n",
    "    return y, x\n",
    "\n",
    "\n",
    "def ols(y, X):\n",
    "    \"\"\"Estimacion por MCC (OLS).\"\"\"\n",
    "    return np.linalg.solve(X.T @ X, X.T @ y)\n",
    "\n",
    "\n",
    "def fgls(y, X, n_total):\n",
    "    \"\"\"\n",
    "    Estimacion por FGLS (MCGE).\n",
    "    1. Estimar OLS y obtener residuos\n",
    "    2. Estimar varianza por grupo con residuos al cuadrado (correccion por g.d.l.)\n",
    "    3. GLS con Omega_hat estimada\n",
    "    Retorna coeficientes y errores estandar.\n",
    "    \"\"\"\n",
    "    N = n_total // N_GROUPS\n",
    "    k = X.shape[1]  # numero de regresores (2: constante + x)\n",
    "\n",
    "    # Paso 1: OLS\n",
    "    beta_ols = ols(y, X)\n",
    "    residuals = y - X @ beta_ols\n",
    "\n",
    "    # Paso 2: Estimar varianzas por grupo con correccion por grados de libertad\n",
    "    # Multiplicamos por n/(n-k) para corregir el sesgo de los residuos OLS\n",
    "    df_correction = n_total / (n_total - k)\n",
    "    sigma2_hat = np.zeros(n_total)\n",
    "    for j in range(N_GROUPS):\n",
    "        idx = slice(j * N, (j + 1) * N)\n",
    "        sigma2_j = np.mean(residuals[idx] ** 2) * df_correction\n",
    "        sigma2_hat[idx] = sigma2_j\n",
    "\n",
    "    # Paso 3: GLS -> beta_hat = (X'Omega_inv X)^{-1} X'Omega_inv y\n",
    "    Omega_inv = np.diag(1.0 / sigma2_hat)\n",
    "    XtOiX = X.T @ Omega_inv @ X\n",
    "    XtOiX_inv = np.linalg.inv(XtOiX)\n",
    "    beta_fgls = XtOiX_inv @ (X.T @ Omega_inv @ y)\n",
    "\n",
    "    # Varianza FGLS: (X'Omega_inv X)^{-1}\n",
    "    se_beta = np.sqrt(np.diag(XtOiX_inv))\n",
    "\n",
    "    return beta_fgls, se_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_simulation_1a(n_total):\n    \"\"\"\n    Simulacion punto 1a para un tamano de muestra dado.\n    FGLS grupos: conoce la estructura de grupos, estima varianzas por grupo.\n    - Tamano del test: H0: beta1 = 0.8 (datos generados con beta1 = 0.8)\n    - Poder del test: datos generados con beta1 = 0 y beta1 = 0.4\n    Usa valores criticos de la normal estandar (distribucion asintotica).\n    \"\"\"\n    beta0_fgls   = np.zeros(N_SIM);  beta1_fgls   = np.zeros(N_SIM)\n    t_size_fgls  = np.zeros(N_SIM)\n    t_pow0_fgls  = np.zeros(N_SIM);  t_pow04_fgls = np.zeros(N_SIM)\n\n    rng = np.random.RandomState(SEED)\n\n    for sim in range(N_SIM):\n        # --- Bajo H0: beta1 = 0.8 ---\n        y, x = generate_data(n_total, BETA0_TRUE, BETA1_TRUE, rng)\n        X = np.column_stack([np.ones(n_total), x])\n        b_f, se_f = fgls(y, X, n_total)\n        beta0_fgls[sim] = b_f[0];  beta1_fgls[sim] = b_f[1]\n        t_size_fgls[sim] = (b_f[1] - BETA1_TRUE) / se_f[1]\n\n        # --- Poder: beta1 = 0 ---\n        y0, x0 = generate_data(n_total, BETA0_TRUE, 0.0, rng)\n        X0 = np.column_stack([np.ones(n_total), x0])\n        b0_f, se0_f = fgls(y0, X0, n_total)\n        t_pow0_fgls[sim] = (b0_f[1] - BETA1_TRUE) / se0_f[1]\n\n        # --- Poder: beta1 = 0.4 ---\n        y04, x04 = generate_data(n_total, BETA0_TRUE, 0.4, rng)\n        X04 = np.column_stack([np.ones(n_total), x04])\n        b04_f, se04_f = fgls(y04, X04, n_total)\n        t_pow04_fgls[sim] = (b04_f[1] - BETA1_TRUE) / se04_f[1]\n\n    cv_1 = stats.norm.ppf(1 - 0.01 / 2)\n    cv_5 = stats.norm.ppf(1 - 0.05 / 2)\n\n    return {\n        'n_total': n_total,\n        'b0_mean': np.mean(beta0_fgls),   'b0_med': np.median(beta0_fgls),   'b0_std': np.std(beta0_fgls),\n        'b1_mean': np.mean(beta1_fgls),   'b1_med': np.median(beta1_fgls),   'b1_std': np.std(beta1_fgls),\n        'size_1':  np.mean(np.abs(t_size_fgls)  > cv_1) * 100,\n        'size_5':  np.mean(np.abs(t_size_fgls)  > cv_5) * 100,\n        'pow0_1':  np.mean(np.abs(t_pow0_fgls)  > cv_1) * 100,\n        'pow0_5':  np.mean(np.abs(t_pow0_fgls)  > cv_5) * 100,\n        'pow04_1': np.mean(np.abs(t_pow04_fgls) > cv_1) * 100,\n        'pow04_5': np.mean(np.abs(t_pow04_fgls) > cv_5) * 100,\n    }\n\n\ndef print_results(res):\n    \"\"\"Imprime resultados FGLS grupos.\"\"\"\n    W = 55\n    print(\"=\" * W)\n    print(f\"  FGLS grupos  |  5N = {res['n_total']}\")\n    print(\"=\" * W)\n\n    print(f\"\\n--- beta0 (verdadero = {BETA0_TRUE}) ---\")\n    print(f\"  Media:   {res['b0_mean']:12.6f}\")\n    print(f\"  Mediana: {res['b0_med']:12.6f}\")\n    print(f\"  Desvio:  {res['b0_std']:12.6f}\")\n\n    print(f\"\\n--- beta1 (verdadero = {BETA1_TRUE}) ---\")\n    print(f\"  Media:   {res['b1_mean']:12.6f}\")\n    print(f\"  Mediana: {res['b1_med']:12.6f}\")\n    print(f\"  Desvio:  {res['b1_std']:12.6f}\")\n\n    print(f\"\\n--- Tamano del test (H0: beta1 = {BETA1_TRUE}) ---\")\n    print(f\"  Al 1%  (nominal: 1%):  {res['size_1']:6.2f}%\")\n    print(f\"  Al 5%  (nominal: 5%):  {res['size_5']:6.2f}%\")\n\n    print(f\"\\n--- Poder del test (H0: beta1 = {BETA1_TRUE}) ---\")\n    print(f\"  beta1=0,   al 1%:  {res['pow0_1']:6.2f}%\")\n    print(f\"  beta1=0,   al 5%:  {res['pow0_5']:6.2f}%\")\n    print(f\"  beta1=0.4, al 1%:  {res['pow04_1']:6.2f}%\")\n    print(f\"  beta1=0.4, al 5%:  {res['pow04_5']:6.2f}%\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "  FGLS - Tamano de muestra: 5N = 5\n",
      "=================================================================\n",
      "\n",
      "--- Estimaciones de beta0 (verdadero = -3.0) ---\n",
      "  Media:    -2.917793\n",
      "  Mediana:  -2.934760\n",
      "  Desvio:   5.597167\n",
      "\n",
      "--- Estimaciones de beta1 (verdadero = 0.8) ---\n",
      "  Media:    0.794782\n",
      "  Mediana:  0.795753\n",
      "  Desvio:   0.192855\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%:  28.64%  (nominal: 1%)\n",
      "  Al 5%:  38.30%  (nominal: 5%)\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1 = 0:    al 1%: 96.96%  |  al 5%: 98.34%\n",
      "  beta1 = 0.4:  al 1%: 80.82%  |  al 5%: 87.68%\n",
      "=================================================================\n",
      "  FGLS - Tamano de muestra: 5N = 10\n",
      "=================================================================\n",
      "\n",
      "--- Estimaciones de beta0 (verdadero = -3.0) ---\n",
      "  Media:    -2.966155\n",
      "  Mediana:  -3.001572\n",
      "  Desvio:   3.034392\n",
      "\n",
      "--- Estimaciones de beta1 (verdadero = 0.8) ---\n",
      "  Media:    0.799587\n",
      "  Mediana:  0.799594\n",
      "  Desvio:   0.105066\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%:  16.28%  (nominal: 1%)\n",
      "  Al 5%:  25.90%  (nominal: 5%)\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1 = 0:    al 1%: 99.96%  |  al 5%: 100.00%\n",
      "  beta1 = 0.4:  al 1%: 95.66%  |  al 5%: 97.90%\n",
      "=================================================================\n",
      "  FGLS - Tamano de muestra: 5N = 30\n",
      "=================================================================\n",
      "\n",
      "--- Estimaciones de beta0 (verdadero = -3.0) ---\n",
      "  Media:    -3.019120\n",
      "  Mediana:  -3.006129\n",
      "  Desvio:   1.393574\n",
      "\n",
      "--- Estimaciones de beta1 (verdadero = 0.8) ---\n",
      "  Media:    0.800590\n",
      "  Mediana:  0.799987\n",
      "  Desvio:   0.048002\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%:  4.38%  (nominal: 1%)\n",
      "  Al 5%:  11.58%  (nominal: 5%)\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1 = 0:    al 1%: 100.00%  |  al 5%: 100.00%\n",
      "  beta1 = 0.4:  al 1%: 100.00%  |  al 5%: 100.00%\n",
      "=================================================================\n",
      "  FGLS - Tamano de muestra: 5N = 100\n",
      "=================================================================\n",
      "\n",
      "--- Estimaciones de beta0 (verdadero = -3.0) ---\n",
      "  Media:    -3.011505\n",
      "  Mediana:  -3.010009\n",
      "  Desvio:   0.700019\n",
      "\n",
      "--- Estimaciones de beta1 (verdadero = 0.8) ---\n",
      "  Media:    0.800362\n",
      "  Mediana:  0.800191\n",
      "  Desvio:   0.023856\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%:  1.78%  (nominal: 1%)\n",
      "  Al 5%:  7.06%  (nominal: 5%)\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1 = 0:    al 1%: 100.00%  |  al 5%: 100.00%\n",
      "  beta1 = 0.4:  al 1%: 100.00%  |  al 5%: 100.00%\n",
      "=================================================================\n",
      "  FGLS - Tamano de muestra: 5N = 200\n",
      "=================================================================\n",
      "\n",
      "--- Estimaciones de beta0 (verdadero = -3.0) ---\n",
      "  Media:    -3.012201\n",
      "  Mediana:  -3.018735\n",
      "  Desvio:   0.481081\n",
      "\n",
      "--- Estimaciones de beta1 (verdadero = 0.8) ---\n",
      "  Media:    0.800345\n",
      "  Mediana:  0.800144\n",
      "  Desvio:   0.016464\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%:  1.40%  (nominal: 1%)\n",
      "  Al 5%:  6.12%  (nominal: 5%)\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1 = 0:    al 1%: 100.00%  |  al 5%: 100.00%\n",
      "  beta1 = 0.4:  al 1%: 100.00%  |  al 5%: 100.00%\n",
      "=================================================================\n",
      "  FGLS - Tamano de muestra: 5N = 500\n",
      "=================================================================\n",
      "\n",
      "--- Estimaciones de beta0 (verdadero = -3.0) ---\n",
      "  Media:    -3.003061\n",
      "  Mediana:  -3.004237\n",
      "  Desvio:   0.294945\n",
      "\n",
      "--- Estimaciones de beta1 (verdadero = 0.8) ---\n",
      "  Media:    0.799976\n",
      "  Mediana:  0.799934\n",
      "  Desvio:   0.010099\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%:  1.04%  (nominal: 1%)\n",
      "  Al 5%:  5.20%  (nominal: 5%)\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1 = 0:    al 1%: 100.00%  |  al 5%: 100.00%\n",
      "  beta1 = 0.4:  al 1%: 100.00%  |  al 5%: 100.00%\n"
     ]
    }
   ],
   "source": [
    "for n in [5,10,30,100,200,500]:\n",
    "    res_5 = run_simulation_1a(n_total=n)\n",
    "    print_results(res_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Punto 1b: Descomposicion de Cholesky\n",
    "\n",
    "Se busca $P$ tal que $\\Omega = P \\cdot P'$. Luego se transforma el modelo:\n",
    "\n",
    "$$P^{-1} y = P^{-1} X \\beta + P^{-1} u$$\n",
    "\n",
    "donde $P^{-1} u$ tiene covarianza $P^{-1} \\Omega (P^{-1})' = P^{-1} P P' (P^{-1})' = I$.\n",
    "\n",
    "Al estimar por OLS el modelo transformado ($y^* = X^* \\beta + u^*$) se obtiene el estimador GLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_simulation_1b(n_total=5):\n    \"\"\"\n    Punto 1b: GLS via Cholesky + OLS sobre modelo transformado.\n    Usa Omega verdadera (conocida), no estimada.\n    Compara con FGLS grupos del punto 1a.\n    \"\"\"\n    N = n_total // N_GROUPS\n\n    # Construir Omega completa (5N x 5N): diagonal con varianzas por grupo\n    omega_full = np.diag(np.repeat(OMEGA_DIAG, N))\n\n    # Descomposicion de Cholesky: Omega = P * P'\n    P = np.linalg.cholesky(omega_full)\n    P_inv = np.linalg.inv(P)\n\n    print(\"Matriz Omega (5x5 para N=1):\")\n    print(omega_full)\n    print(\"\\nMatriz P (Cholesky, Omega = P * P'):\")\n    print(P)\n    print(\"\\nVerificacion P * P' = Omega:\", np.allclose(P @ P.T, omega_full))\n\n    # Simulacion\n    beta0_gls  = np.zeros(N_SIM);  beta1_gls  = np.zeros(N_SIM)\n    beta0_fgls = np.zeros(N_SIM);  beta1_fgls = np.zeros(N_SIM)\n\n    rng = np.random.RandomState(SEED)\n\n    for sim in range(N_SIM):\n        y, x = generate_data(n_total, BETA0_TRUE, BETA1_TRUE, rng)\n        X = np.column_stack([np.ones(n_total), x])\n\n        # --- GLS via Cholesky (oracle): transformar y estimar OLS ---\n        y_star = P_inv @ y\n        X_star = P_inv @ X\n        beta_cholesky = ols(y_star, X_star)\n        beta0_gls[sim] = beta_cholesky[0]\n        beta1_gls[sim] = beta_cholesky[1]\n\n        # --- FGLS grupos (punto 1a) ---\n        beta_fgls_hat, _ = fgls(y, X, n_total)\n        beta0_fgls[sim] = beta_fgls_hat[0]\n        beta1_fgls[sim] = beta_fgls_hat[1]\n\n    # Reportar comparacion\n    W = 65\n    print(\"\\n\" + \"=\" * W)\n    print(f\"  GLS (Cholesky, oracle) vs FGLS grupos  |  5N = {n_total}\")\n    print(\"=\" * W)\n\n    print(\"\\n--- beta0 (verdadero = -3.0) ---\")\n    print(f\"  {'':20s} {'GLS (Cholesky)':>16s} {'FGLS grupos':>14s}\")\n    print(f\"  {'Media':20s} {np.mean(beta0_gls):16.6f} {np.mean(beta0_fgls):14.6f}\")\n    print(f\"  {'Mediana':20s} {np.median(beta0_gls):16.6f} {np.median(beta0_fgls):14.6f}\")\n    print(f\"  {'Desvio':20s} {np.std(beta0_gls):16.6f} {np.std(beta0_fgls):14.6f}\")\n\n    print(\"\\n--- beta1 (verdadero = 0.8) ---\")\n    print(f\"  {'':20s} {'GLS (Cholesky)':>16s} {'FGLS grupos':>14s}\")\n    print(f\"  {'Media':20s} {np.mean(beta1_gls):16.6f} {np.mean(beta1_fgls):14.6f}\")\n    print(f\"  {'Mediana':20s} {np.median(beta1_gls):16.6f} {np.median(beta1_fgls):14.6f}\")\n    print(f\"  {'Desvio':20s} {np.std(beta1_gls):16.6f} {np.std(beta1_fgls):14.6f}\")\n\nrun_simulation_1b(n_total=5)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1a) Diseno 0 (tamano) + 2.1b) Disenos 1 y 2 (poder) para todos los n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "  Test de White - n = 20\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 0.48%  (nominal:  1%)\n",
      "    Al  5%: 4.60%  (nominal:  5%)\n",
      "    Al 10%: 9.04%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 0.60%\n",
      "    Al  5%: 6.00%\n",
      "    Al 10%: 11.38%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 0.76%\n",
      "    Al  5%: 6.92%\n",
      "    Al 10%: 12.26%\n",
      "\n",
      "=================================================================\n",
      "  Test de White - n = 60\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 0.90%  (nominal:  1%)\n",
      "    Al  5%: 4.86%  (nominal:  5%)\n",
      "    Al 10%: 9.94%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 5.14%\n",
      "    Al  5%: 14.66%\n",
      "    Al 10%: 23.86%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 3.44%\n",
      "    Al  5%: 11.20%\n",
      "    Al 10%: 18.06%\n",
      "\n",
      "=================================================================\n",
      "  Test de White - n = 100\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 1.08%  (nominal:  1%)\n",
      "    Al  5%: 4.70%  (nominal:  5%)\n",
      "    Al 10%: 9.78%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 11.04%\n",
      "    Al  5%: 26.66%\n",
      "    Al 10%: 39.46%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 5.06%\n",
      "    Al  5%: 15.10%\n",
      "    Al 10%: 22.88%\n",
      "\n",
      "=================================================================\n",
      "  Test de White - n = 200\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 0.96%  (nominal:  1%)\n",
      "    Al  5%: 4.88%  (nominal:  5%)\n",
      "    Al 10%: 9.42%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 33.44%\n",
      "    Al  5%: 58.92%\n",
      "    Al 10%: 72.06%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 11.24%\n",
      "    Al  5%: 26.82%\n",
      "    Al 10%: 38.78%\n",
      "\n",
      "=================================================================\n",
      "  Test de White - n = 400\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 1.02%  (nominal:  1%)\n",
      "    Al  5%: 5.34%  (nominal:  5%)\n",
      "    Al 10%: 9.52%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 79.92%\n",
      "    Al  5%: 93.86%\n",
      "    Al 10%: 97.26%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 28.48%\n",
      "    Al  5%: 52.40%\n",
      "    Al 10%: 64.68%\n",
      "\n",
      "=================================================================\n",
      "  Test de White - n = 600\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 1.24%  (nominal:  1%)\n",
      "    Al  5%: 4.90%  (nominal:  5%)\n",
      "    Al 10%: 10.28%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 97.52%\n",
      "    Al  5%: 99.62%\n",
      "    Al 10%: 99.86%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 47.88%\n",
      "    Al  5%: 71.14%\n",
      "    Al 10%: 80.78%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Correr test de White para todos los tamanos de muestra y los 3 disenos\n",
    "sample_sizes_p2 = [20, 60, 100, 200, 400, 600]\n",
    "\n",
    "for n in sample_sizes_p2:\n",
    "    res = run_white_test_simulation(n, designs=[0, 1, 2])\n",
    "    print_white_results(res, n)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Correccion de la matriz de varianzas y covarianzas (White)\n",
    "\n",
    "Sesgo relativo: $b_j = \\frac{1}{S}\\sum_{s=1}^{S} \\frac{\\widehat{Var}(\\hat\\beta_j)^{(s)} - Var(\\hat\\beta_j)}{Var(\\hat\\beta_j)}$\n",
    "\n",
    "Sesgo relativo total: $|b_0| + |b_1| + |b_2|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  Sesgo relativo de White (residuos OLS)\n",
      "======================================================================\n",
      "\n",
      "--- Diseno 1 ---\n",
      "      n          b0          b1          b2    |b0|+|b1|+|b2|\n",
      "     20     -0.1592     -0.2099     -0.2123            0.5813\n",
      "     60     -0.0511     -0.0677     -0.0644            0.1831\n",
      "    100     -0.0317     -0.0430     -0.0407            0.1154\n",
      "    200     -0.0160     -0.0204     -0.0204            0.0568\n",
      "    400     -0.0070     -0.0093     -0.0088            0.0251\n",
      "    600     -0.0048     -0.0055     -0.0067            0.0171\n",
      "\n",
      "--- Diseno 2 ---\n",
      "      n          b0          b1          b2    |b0|+|b1|+|b2|\n",
      "     20     -0.1546     -0.2071     -0.2226            0.5843\n",
      "     60     -0.0614     -0.0730     -0.0790            0.2133\n",
      "    100     -0.0364     -0.0443     -0.0466            0.1273\n",
      "    200     -0.0182     -0.0204     -0.0250            0.0636\n",
      "    400     -0.0094     -0.0093     -0.0140            0.0327\n",
      "    600     -0.0049     -0.0036     -0.0088            0.0173\n"
     ]
    }
   ],
   "source": [
    "def white_variance_simulation(n, design, use_true_errors=False, rng_seed=SEED):\n",
    "    \"\"\"\n",
    "    Parte 2.2: Sesgo relativo de la estimacion de White de la matriz de var-cov.\n",
    "    \n",
    "    Var_true = (X'X)^{-1} X' Omega_true X (X'X)^{-1}  (varianza verdadera de OLS)\n",
    "    Var_hat  = (X'X)^{-1} X' Omega_hat  X (X'X)^{-1}  (estimacion de White)\n",
    "    \n",
    "    Omega_hat = diag(e_hat^2) si use_true_errors=False (residuos)\n",
    "    Omega_hat = diag(epsilon^2) si use_true_errors=True (errores verdaderos)\n",
    "    \"\"\"\n",
    "    rng_x = np.random.RandomState(rng_seed)\n",
    "    x1, x2 = generate_x_part2(n, rng_x)\n",
    "    X = np.column_stack([np.ones(n), x1, x2])\n",
    "    \n",
    "    # Varianza de u segun diseno\n",
    "    if design == 1:\n",
    "        var_u = 1.0  # u ~ N(0,1)\n",
    "    elif design == 2:\n",
    "        var_u = 5.0 / 3.0  # u ~ t5, Var(t5) = 5/(5-2)\n",
    "    \n",
    "    # nu_i para heterocedasticidad\n",
    "    nu = np.exp(0.25 * x1 + 0.25 * x2)\n",
    "    \n",
    "    # Varianza verdadera de epsilon_i = sqrt(nu_i)*u_i es: nu_i * Var(u)\n",
    "    sigma2_true = nu * var_u\n",
    "    Omega_true = np.diag(sigma2_true)\n",
    "    \n",
    "    # Varianza verdadera de beta_hat OLS: (X'X)^{-1} X' Omega_true X (X'X)^{-1}\n",
    "    XtX_inv = np.linalg.inv(X.T @ X)\n",
    "    Var_true = XtX_inv @ X.T @ Omega_true @ X @ XtX_inv\n",
    "    var_true_diag = np.diag(Var_true)  # [Var(b0), Var(b1), Var(b2)]\n",
    "    \n",
    "    # Simulacion\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    relative_bias = np.zeros((N_SIM, 3))  # para b0, b1, b2\n",
    "    \n",
    "    for sim in range(N_SIM):\n",
    "        # Generar errores\n",
    "        if design == 1:\n",
    "            u = rng.normal(0, 1, size=n)\n",
    "        elif design == 2:\n",
    "            u = rng.standard_t(df=5, size=n)\n",
    "        \n",
    "        epsilon = np.sqrt(nu) * u\n",
    "        y = 1 + 1 * x1 + 1 * x2 + epsilon\n",
    "        \n",
    "        # OLS\n",
    "        beta_hat = ols(y, X)\n",
    "        e_hat = y - X @ beta_hat\n",
    "        \n",
    "        # Estimacion de White\n",
    "        if use_true_errors:\n",
    "            Omega_hat = np.diag(epsilon ** 2)  # errores verdaderos\n",
    "        else:\n",
    "            Omega_hat = np.diag(e_hat ** 2)    # residuos OLS\n",
    "        \n",
    "        Var_hat = XtX_inv @ X.T @ Omega_hat @ X @ XtX_inv\n",
    "        var_hat_diag = np.diag(Var_hat)\n",
    "        \n",
    "        # Sesgo relativo para esta simulacion\n",
    "        relative_bias[sim, :] = (var_hat_diag - var_true_diag) / var_true_diag\n",
    "    \n",
    "    # Promediar sobre simulaciones\n",
    "    b = np.mean(relative_bias, axis=0)  # b0, b1, b2\n",
    "    total_bias = np.sum(np.abs(b))\n",
    "    \n",
    "    return b, total_bias\n",
    "\n",
    "\n",
    "def run_white_variance_all(use_true_errors=False):\n",
    "    \"\"\"Corre la simulacion 2.2 para todos los n y disenos.\"\"\"\n",
    "    sample_sizes = [20, 60, 100, 200, 400, 600]\n",
    "    label = \"errores verdaderos\" if use_true_errors else \"residuos OLS\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Sesgo relativo de White ({label})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for design in [1, 2]:\n",
    "        print(f\"\\n--- Diseno {design} ---\")\n",
    "        print(f\"  {'n':>5s}  {'b0':>10s}  {'b1':>10s}  {'b2':>10s}  {'|b0|+|b1|+|b2|':>16s}\")\n",
    "        \n",
    "        for n in sample_sizes:\n",
    "            b, total = white_variance_simulation(n, design, use_true_errors)\n",
    "            print(f\"  {n:5d}  {b[0]:10.4f}  {b[1]:10.4f}  {b[2]:10.4f}  {total:16.4f}\")\n",
    "\n",
    "\n",
    "# a-f) Con residuos OLS (estimacion estandar de White)\n",
    "run_white_variance_all(use_true_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2g) Repetir con errores verdaderos en vez de residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  Sesgo relativo de White (errores verdaderos)\n",
      "======================================================================\n",
      "\n",
      "--- Diseno 1 ---\n",
      "      n          b0          b1          b2    |b0|+|b1|+|b2|\n",
      "     20      0.0018     -0.0008      0.0144            0.0169\n",
      "     60      0.0006      0.0011      0.0069            0.0086\n",
      "    100     -0.0006     -0.0015      0.0031            0.0052\n",
      "    200     -0.0003      0.0004      0.0014            0.0021\n",
      "    400      0.0008      0.0008      0.0019            0.0034\n",
      "    600      0.0004      0.0013      0.0005            0.0023\n",
      "\n",
      "--- Diseno 2 ---\n",
      "      n          b0          b1          b2    |b0|+|b1|+|b2|\n",
      "     20     -0.0018     -0.0109     -0.0158            0.0286\n",
      "     60     -0.0097     -0.0067     -0.0086            0.0250\n",
      "    100     -0.0050     -0.0039     -0.0036            0.0125\n",
      "    200     -0.0025     -0.0006     -0.0042            0.0073\n",
      "    400     -0.0017      0.0005     -0.0035            0.0057\n",
      "    600      0.0003      0.0030     -0.0019            0.0052\n"
     ]
    }
   ],
   "source": [
    "# g) Con errores verdaderos (epsilon^2 en vez de e_hat^2)\n",
    "run_white_variance_all(use_true_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}