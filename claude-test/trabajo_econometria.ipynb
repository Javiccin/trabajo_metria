{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Practico de Econometria\n",
    "**Maestrias en Economia y Econometria**\n",
    "\n",
    "Seed: 1910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Seed para replicabilidad\n",
    "SEED = 1910\n",
    "\n",
    "# Parametros del modelo\n",
    "N_SIM = 5000\n",
    "BETA0_TRUE = -3.0\n",
    "BETA1_TRUE = 0.8\n",
    "OMEGA_DIAG = np.array([4.0, 9.0, 16.0, 25.0, 36.0])  # varianzas por grupo\n",
    "N_GROUPS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Propiedades de muestra finita de FGLS (MCGE)\n",
    "\n",
    "**Modelo:** $y_i = \\beta_0 + \\beta_1 x_i + u_i$, con $\\beta_0 = -3$, $\\beta_1 = 0.8$\n",
    "\n",
    "$u \\sim N(0, \\Omega \\otimes I_N)$, $\\Omega = \\text{diag}(4, 9, 16, 25, 36)$, $x \\sim U[1, 50]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_data(n_total, beta0, beta1, rng):\n    \"\"\"\n    Genera una muestra de tamano n_total = 5*N del modelo (1).\n    Cada grupo j (j=0,...,4) tiene N = n_total/5 observaciones con varianza Omega[j,j].\n    \"\"\"\n    N = n_total // N_GROUPS\n    x = rng.uniform(1, 50, size=n_total)\n\n    # Errores con heterocedasticidad grupal\n    u = np.zeros(n_total)\n    for j in range(N_GROUPS):\n        sigma_j = np.sqrt(OMEGA_DIAG[j])\n        u[j * N:(j + 1) * N] = rng.normal(0, sigma_j, size=N)\n\n    y = beta0 + beta1 * x + u\n    return y, x\n\n\ndef ols(y, X):\n    \"\"\"Estimacion por MCC (OLS).\"\"\"\n    return np.linalg.solve(X.T @ X, X.T @ y)\n\n\ndef fgls(y, X, n_total):\n    \"\"\"\n    Estimacion por FGLS (MCGE).\n    1. Estimar OLS y obtener residuos\n    2. Estimar varianza por grupo como promedio simple de residuos al cuadrado\n    3. GLS con Omega_hat estimada\n    Retorna coeficientes y errores estandar.\n    \"\"\"\n    N = n_total // N_GROUPS\n\n    # Paso 1: OLS\n    beta_ols = ols(y, X)\n    residuals = y - X @ beta_ols\n\n    # Paso 2: Estimar varianzas por grupo como promedio simple de residuos al cuadrado\n    sigma2_hat = np.zeros(n_total)\n    for j in range(N_GROUPS):\n        idx = slice(j * N, (j + 1) * N)\n        sigma2_j = np.mean(residuals[idx] ** 2)\n        sigma2_hat[idx] = sigma2_j\n\n    # Paso 3: GLS -> beta_hat = (X'Omega_inv X)^{-1} X'Omega_inv y\n    Omega_inv = np.diag(1.0 / sigma2_hat)\n    XtOiX = X.T @ Omega_inv @ X\n    XtOiX_inv = np.linalg.inv(XtOiX)\n    beta_fgls = XtOiX_inv @ (X.T @ Omega_inv @ y)\n\n    # Varianza FGLS: (X'Omega_inv X)^{-1}\n    se_beta = np.sqrt(np.diag(XtOiX_inv))\n\n    return beta_fgls, se_beta\n\n\ndef fgls_white(y, X):\n    \"\"\"\n    FGLS a la White (sin conocer la estructura de errores).\n    1. OLS -> residuos u_hat\n    2. Regresion auxiliar: u_hat^2 ~ 1 + x + x^2\n    3. sigma2_hat_i = valores ajustados (truncados a > 0)\n    4. WLS con pesos w_i = 1/sigma2_hat_i\n    Retorna coeficientes y errores estandar.\n    \"\"\"\n    n = X.shape[0]\n    x = X[:, 1]\n\n    # Paso 1: OLS y residuos\n    beta_ols = ols(y, X)\n    u2 = (y - X @ beta_ols) ** 2\n\n    # Paso 2: Regresion auxiliar\n    Z = np.column_stack([np.ones(n), x, x**2])\n    gamma_hat = np.linalg.solve(Z.T @ Z, Z.T @ u2)\n    sigma2_hat = Z @ gamma_hat\n\n    # Truncar valores no positivos\n    sigma2_hat = np.maximum(sigma2_hat, 1e-8)\n\n    # Paso 3: WLS = GLS con Omega_hat = diag(sigma2_hat)\n    Omega_inv = np.diag(1.0 / sigma2_hat)\n    XtOiX = X.T @ Omega_inv @ X\n    XtOiX_inv = np.linalg.inv(XtOiX)\n    beta_white = XtOiX_inv @ (X.T @ Omega_inv @ y)\n    se_beta = np.sqrt(np.diag(XtOiX_inv))\n\n    return beta_white, se_beta"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_simulation_1a(n_total):\n    \"\"\"\n    Simulacion punto 1a para un tamano de muestra dado.\n    Compara FGLS grupos (conoce estructura) vs FGLS White (regresion auxiliar).\n    - Tamano del test: H0: beta1 = 0.8 (datos generados con beta1 = 0.8)\n    - Poder del test: datos generados con beta1 = 0 y beta1 = 0.4\n    Usa valores criticos de la normal estandar (distribucion asintotica).\n    \"\"\"\n    # FGLS grupos\n    beta0_fgls   = np.zeros(N_SIM);  beta1_fgls   = np.zeros(N_SIM)\n    t_size_fgls  = np.zeros(N_SIM)\n    t_pow0_fgls  = np.zeros(N_SIM);  t_pow04_fgls = np.zeros(N_SIM)\n\n    # FGLS White\n    beta0_w   = np.zeros(N_SIM);  beta1_w   = np.zeros(N_SIM)\n    t_size_w  = np.zeros(N_SIM)\n    t_pow0_w  = np.zeros(N_SIM);  t_pow04_w = np.zeros(N_SIM)\n\n    rng = np.random.RandomState(SEED)\n\n    for sim in range(N_SIM):\n        # --- Bajo H0: beta1 = 0.8 ---\n        y, x = generate_data(n_total, BETA0_TRUE, BETA1_TRUE, rng)\n        X = np.column_stack([np.ones(n_total), x])\n        b_f, se_f = fgls(y, X, n_total)\n        beta0_fgls[sim] = b_f[0];  beta1_fgls[sim] = b_f[1]\n        t_size_fgls[sim] = (b_f[1] - BETA1_TRUE) / se_f[1]\n\n        b_w, se_w = fgls_white(y, X)\n        beta0_w[sim] = b_w[0];  beta1_w[sim] = b_w[1]\n        t_size_w[sim] = (b_w[1] - BETA1_TRUE) / se_w[1]\n\n        # --- Poder: beta1 = 0 ---\n        y0, x0 = generate_data(n_total, BETA0_TRUE, 0.0, rng)\n        X0 = np.column_stack([np.ones(n_total), x0])\n        b0_f, se0_f = fgls(y0, X0, n_total)\n        t_pow0_fgls[sim] = (b0_f[1] - BETA1_TRUE) / se0_f[1]\n\n        b0_w, se0_w = fgls_white(y0, X0)\n        t_pow0_w[sim] = (b0_w[1] - BETA1_TRUE) / se0_w[1]\n\n        # --- Poder: beta1 = 0.4 ---\n        y04, x04 = generate_data(n_total, BETA0_TRUE, 0.4, rng)\n        X04 = np.column_stack([np.ones(n_total), x04])\n        b04_f, se04_f = fgls(y04, X04, n_total)\n        t_pow04_fgls[sim] = (b04_f[1] - BETA1_TRUE) / se04_f[1]\n\n        b04_w, se04_w = fgls_white(y04, X04)\n        t_pow04_w[sim] = (b04_w[1] - BETA1_TRUE) / se04_w[1]\n\n    cv_1 = stats.norm.ppf(1 - 0.01 / 2)\n    cv_5 = stats.norm.ppf(1 - 0.05 / 2)\n\n    return {\n        'n_total': n_total,\n        # FGLS grupos\n        'b0_mean': np.mean(beta0_fgls),   'b0_med': np.median(beta0_fgls),   'b0_std': np.std(beta0_fgls),\n        'b1_mean': np.mean(beta1_fgls),   'b1_med': np.median(beta1_fgls),   'b1_std': np.std(beta1_fgls),\n        'size_1':  np.mean(np.abs(t_size_fgls)  > cv_1) * 100,\n        'size_5':  np.mean(np.abs(t_size_fgls)  > cv_5) * 100,\n        'pow0_1':  np.mean(np.abs(t_pow0_fgls)  > cv_1) * 100,\n        'pow0_5':  np.mean(np.abs(t_pow0_fgls)  > cv_5) * 100,\n        'pow04_1': np.mean(np.abs(t_pow04_fgls) > cv_1) * 100,\n        'pow04_5': np.mean(np.abs(t_pow04_fgls) > cv_5) * 100,\n        # FGLS White\n        'b0_mean_w': np.mean(beta0_w),   'b0_med_w': np.median(beta0_w),   'b0_std_w': np.std(beta0_w),\n        'b1_mean_w': np.mean(beta1_w),   'b1_med_w': np.median(beta1_w),   'b1_std_w': np.std(beta1_w),\n        'size_1_w':  np.mean(np.abs(t_size_w)  > cv_1) * 100,\n        'size_5_w':  np.mean(np.abs(t_size_w)  > cv_5) * 100,\n        'pow0_1_w':  np.mean(np.abs(t_pow0_w)  > cv_1) * 100,\n        'pow0_5_w':  np.mean(np.abs(t_pow0_w)  > cv_5) * 100,\n        'pow04_1_w': np.mean(np.abs(t_pow04_w) > cv_1) * 100,\n        'pow04_5_w': np.mean(np.abs(t_pow04_w) > cv_5) * 100,\n    }\n\n\ndef print_results(res):\n    \"\"\"Imprime resultados FGLS grupos vs FGLS White lado a lado.\"\"\"\n    W = 57\n    print(\"=\" * W)\n    print(f\"  5N = {res['n_total']}\")\n    print(\"=\" * W)\n    print(f\"  {'':30s} {'FGLS grupos':>12s}  {'FGLS White':>12s}\")\n\n    print(f\"\\n--- beta0 (verdadero = {BETA0_TRUE}) ---\")\n    print(f\"  {'Media':30s} {res['b0_mean']:12.6f}  {res['b0_mean_w']:12.6f}\")\n    print(f\"  {'Mediana':30s} {res['b0_med']:12.6f}  {res['b0_med_w']:12.6f}\")\n    print(f\"  {'Desvio':30s} {res['b0_std']:12.6f}  {res['b0_std_w']:12.6f}\")\n\n    print(f\"\\n--- beta1 (verdadero = {BETA1_TRUE}) ---\")\n    print(f\"  {'Media':30s} {res['b1_mean']:12.6f}  {res['b1_mean_w']:12.6f}\")\n    print(f\"  {'Mediana':30s} {res['b1_med']:12.6f}  {res['b1_med_w']:12.6f}\")\n    print(f\"  {'Desvio':30s} {res['b1_std']:12.6f}  {res['b1_std_w']:12.6f}\")\n\n    print(f\"\\n--- Tamano del test (H0: beta1 = {BETA1_TRUE}) ---\")\n    print(f\"  {'Al 1%  (nominal: 1%)':30s} {res['size_1']:11.2f}%  {res['size_1_w']:11.2f}%\")\n    print(f\"  {'Al 5%  (nominal: 5%)':30s} {res['size_5']:11.2f}%  {res['size_5_w']:11.2f}%\")\n\n    print(f\"\\n--- Poder del test (H0: beta1 = {BETA1_TRUE}) ---\")\n    print(f\"  {'beta1=0,   al 1%':30s} {res['pow0_1']:11.2f}%  {res['pow0_1_w']:11.2f}%\")\n    print(f\"  {'beta1=0,   al 5%':30s} {res['pow0_5']:11.2f}%  {res['pow0_5_w']:11.2f}%\")\n    print(f\"  {'beta1=0.4, al 1%':30s} {res['pow04_1']:11.2f}%  {res['pow04_1_w']:11.2f}%\")\n    print(f\"  {'beta1=0.4, al 5%':30s} {res['pow04_5']:11.2f}%  {res['pow04_5_w']:11.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "  FGLS grupos  |  5N = 5\n",
      "=======================================================\n",
      "\n",
      "--- beta0 (verdadero = -3.0) ---\n",
      "  Media:      -2.917793\n",
      "  Mediana:    -2.934760\n",
      "  Desvio:      5.597167\n",
      "\n",
      "--- beta1 (verdadero = 0.8) ---\n",
      "  Media:       0.794782\n",
      "  Mediana:     0.795753\n",
      "  Desvio:      0.192855\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%  (nominal: 1%):   37.68%\n",
      "  Al 5%  (nominal: 5%):   47.56%\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1=0,   al 1%:   98.28%\n",
      "  beta1=0,   al 5%:   99.04%\n",
      "  beta1=0.4, al 1%:   87.42%\n",
      "  beta1=0.4, al 5%:   91.52%\n",
      "=======================================================\n",
      "  FGLS grupos  |  5N = 10\n",
      "=======================================================\n",
      "\n",
      "--- beta0 (verdadero = -3.0) ---\n",
      "  Media:      -2.966155\n",
      "  Mediana:    -3.001572\n",
      "  Desvio:      3.034392\n",
      "\n",
      "--- beta1 (verdadero = 0.8) ---\n",
      "  Media:       0.799587\n",
      "  Mediana:     0.799594\n",
      "  Desvio:      0.105066\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%  (nominal: 1%):   19.64%\n",
      "  Al 5%  (nominal: 5%):   30.08%\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1=0,   al 1%:   99.98%\n",
      "  beta1=0,   al 5%:  100.00%\n",
      "  beta1=0.4, al 1%:   96.90%\n",
      "  beta1=0.4, al 5%:   98.22%\n",
      "=======================================================\n",
      "  FGLS grupos  |  5N = 30\n",
      "=======================================================\n",
      "\n",
      "--- beta0 (verdadero = -3.0) ---\n",
      "  Media:      -3.019120\n",
      "  Mediana:    -3.006129\n",
      "  Desvio:      1.393574\n",
      "\n",
      "--- beta1 (verdadero = 0.8) ---\n",
      "  Media:       0.800590\n",
      "  Mediana:     0.799987\n",
      "  Desvio:      0.048002\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%  (nominal: 1%):    5.18%\n",
      "  Al 5%  (nominal: 5%):   12.70%\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1=0,   al 1%:  100.00%\n",
      "  beta1=0,   al 5%:  100.00%\n",
      "  beta1=0.4, al 1%:  100.00%\n",
      "  beta1=0.4, al 5%:  100.00%\n",
      "=======================================================\n",
      "  FGLS grupos  |  5N = 100\n",
      "=======================================================\n",
      "\n",
      "--- beta0 (verdadero = -3.0) ---\n",
      "  Media:      -3.011505\n",
      "  Mediana:    -3.010009\n",
      "  Desvio:      0.700019\n",
      "\n",
      "--- beta1 (verdadero = 0.8) ---\n",
      "  Media:       0.800362\n",
      "  Mediana:     0.800191\n",
      "  Desvio:      0.023856\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%  (nominal: 1%):    1.90%\n",
      "  Al 5%  (nominal: 5%):    7.24%\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1=0,   al 1%:  100.00%\n",
      "  beta1=0,   al 5%:  100.00%\n",
      "  beta1=0.4, al 1%:  100.00%\n",
      "  beta1=0.4, al 5%:  100.00%\n",
      "=======================================================\n",
      "  FGLS grupos  |  5N = 200\n",
      "=======================================================\n",
      "\n",
      "--- beta0 (verdadero = -3.0) ---\n",
      "  Media:      -3.012201\n",
      "  Mediana:    -3.018735\n",
      "  Desvio:      0.481081\n",
      "\n",
      "--- beta1 (verdadero = 0.8) ---\n",
      "  Media:       0.800345\n",
      "  Mediana:     0.800144\n",
      "  Desvio:      0.016464\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%  (nominal: 1%):    1.48%\n",
      "  Al 5%  (nominal: 5%):    6.28%\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1=0,   al 1%:  100.00%\n",
      "  beta1=0,   al 5%:  100.00%\n",
      "  beta1=0.4, al 1%:  100.00%\n",
      "  beta1=0.4, al 5%:  100.00%\n",
      "=======================================================\n",
      "  FGLS grupos  |  5N = 500\n",
      "=======================================================\n",
      "\n",
      "--- beta0 (verdadero = -3.0) ---\n",
      "  Media:      -3.003061\n",
      "  Mediana:    -3.004237\n",
      "  Desvio:      0.294945\n",
      "\n",
      "--- beta1 (verdadero = 0.8) ---\n",
      "  Media:       0.799976\n",
      "  Mediana:     0.799934\n",
      "  Desvio:      0.010099\n",
      "\n",
      "--- Tamano del test (H0: beta1 = 0.8) ---\n",
      "  Al 1%  (nominal: 1%):    1.04%\n",
      "  Al 5%  (nominal: 5%):    5.24%\n",
      "\n",
      "--- Poder del test (H0: beta1 = 0.8) ---\n",
      "  beta1=0,   al 1%:  100.00%\n",
      "  beta1=0,   al 5%:  100.00%\n",
      "  beta1=0.4, al 1%:  100.00%\n",
      "  beta1=0.4, al 5%:  100.00%\n"
     ]
    }
   ],
   "source": [
    "for n in [5,10,30,100,200,500]:\n",
    "    res_5 = run_simulation_1a(n_total=n)\n",
    "    print_results(res_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1b: Descomposicion de Cholesky\n",
    "\n",
    "Se busca $P$ tal que $\\Omega = P \\cdot P'$. Luego se transforma el modelo:\n",
    "\n",
    "$$P^{-1} y = P^{-1} X \\beta + P^{-1} u$$\n",
    "\n",
    "donde $P^{-1} u$ tiene covarianza $P^{-1} \\Omega (P^{-1})' = P^{-1} P P' (P^{-1})' = I$.\n",
    "\n",
    "Al estimar por OLS el modelo transformado ($y^* = X^* \\beta + u^*$) se obtiene el estimador GLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_simulation_1b(n_total=5):\n    \"\"\"\n    Punto 1b: GLS via Cholesky + OLS sobre modelo transformado.\n    Usa Omega verdadera (conocida), no estimada.\n    Compara GLS (Cholesky, oracle) vs FGLS grupos vs FGLS White.\n    \"\"\"\n    N = n_total // N_GROUPS\n\n    # Construir Omega completa (5N x 5N): diagonal con varianzas por grupo\n    omega_full = np.diag(np.repeat(OMEGA_DIAG, N))\n\n    # Descomposicion de Cholesky: Omega = P * P'\n    P = np.linalg.cholesky(omega_full)\n    P_inv = np.linalg.inv(P)\n\n    print(\"Matriz Omega (5x5 para N=1):\")\n    print(omega_full)\n    print(\"\\nMatriz P (Cholesky, Omega = P * P'):\")\n    print(P)\n    print(\"\\nVerificacion P * P' = Omega:\", np.allclose(P @ P.T, omega_full))\n\n    # Simulacion\n    beta0_gls   = np.zeros(N_SIM);  beta1_gls   = np.zeros(N_SIM)\n    beta0_fgls  = np.zeros(N_SIM);  beta1_fgls  = np.zeros(N_SIM)\n    beta0_white = np.zeros(N_SIM);  beta1_white = np.zeros(N_SIM)\n\n    rng = np.random.RandomState(SEED)\n\n    for sim in range(N_SIM):\n        y, x = generate_data(n_total, BETA0_TRUE, BETA1_TRUE, rng)\n        X = np.column_stack([np.ones(n_total), x])\n\n        # --- GLS via Cholesky (oracle): transformar y estimar OLS ---\n        y_star = P_inv @ y\n        X_star = P_inv @ X\n        beta_cholesky = ols(y_star, X_star)\n        beta0_gls[sim] = beta_cholesky[0]\n        beta1_gls[sim] = beta_cholesky[1]\n\n        # --- FGLS grupos (punto 1a) ---\n        beta_fgls_hat, _ = fgls(y, X, n_total)\n        beta0_fgls[sim] = beta_fgls_hat[0]\n        beta1_fgls[sim] = beta_fgls_hat[1]\n\n        # --- FGLS White ---\n        beta_white_hat, _ = fgls_white(y, X)\n        beta0_white[sim] = beta_white_hat[0]\n        beta1_white[sim] = beta_white_hat[1]\n\n    # Reportar comparacion\n    W = 75\n    print(\"\\n\" + \"=\" * W)\n    print(f\"  GLS (Cholesky) vs FGLS grupos vs FGLS White  |  5N = {n_total}\")\n    print(\"=\" * W)\n\n    print(\"\\n--- beta0 (verdadero = -3.0) ---\")\n    print(f\"  {'':20s} {'GLS (Cholesky)':>16s} {'FGLS grupos':>14s} {'FGLS White':>12s}\")\n    print(f\"  {'Media':20s} {np.mean(beta0_gls):16.6f} {np.mean(beta0_fgls):14.6f} {np.mean(beta0_white):12.6f}\")\n    print(f\"  {'Mediana':20s} {np.median(beta0_gls):16.6f} {np.median(beta0_fgls):14.6f} {np.median(beta0_white):12.6f}\")\n    print(f\"  {'Desvio':20s} {np.std(beta0_gls):16.6f} {np.std(beta0_fgls):14.6f} {np.std(beta0_white):12.6f}\")\n\n    print(\"\\n--- beta1 (verdadero = 0.8) ---\")\n    print(f\"  {'':20s} {'GLS (Cholesky)':>16s} {'FGLS grupos':>14s} {'FGLS White':>12s}\")\n    print(f\"  {'Media':20s} {np.mean(beta1_gls):16.6f} {np.mean(beta1_fgls):14.6f} {np.mean(beta1_white):12.6f}\")\n    print(f\"  {'Mediana':20s} {np.median(beta1_gls):16.6f} {np.median(beta1_fgls):14.6f} {np.median(beta1_white):12.6f}\")\n    print(f\"  {'Desvio':20s} {np.std(beta1_gls):16.6f} {np.std(beta1_fgls):14.6f} {np.std(beta1_white):12.6f}\")\n\nrun_simulation_1b(n_total=5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Generacion de x1 y x2 para la Parte 2\n",
    "# ============================================================================\n",
    "\n",
    "def generate_x_part2(n, rng):\n",
    "    \"\"\"\n",
    "    Genera x1 y x2 para el modelo (2).\n",
    "    Base (n=20): x1 = 18 puntos equiespaciados entre -1 y 1 + extremos -1.1 y 1.1\n",
    "                 x2 = cuantiles de N(0,1) aleatorios\n",
    "    Para n > 20: se repiten las 20 observaciones base las veces necesarias.\n",
    "    \"\"\"\n",
    "    # Base de 20 observaciones\n",
    "    x1_interior = np.linspace(-1, 1, 18)\n",
    "    x1_base = np.concatenate([[-1.1], x1_interior, [1.1]])  # 20 puntos\n",
    "    x2_base = rng.normal(0, 1, size=20)\n",
    "\n",
    "    # Repetir para obtener n observaciones\n",
    "    reps = n // 20\n",
    "    x1 = np.tile(x1_base, reps)\n",
    "    x2 = np.tile(x2_base, reps)\n",
    "\n",
    "    return x1, x2\n",
    "\n",
    "\n",
    "def generate_y_part2(x1, x2, design, rng):\n",
    "    \"\"\"\n",
    "    Genera y del modelo (2) segun el diseno.\n",
    "    y_i = 1 + 1*x1_i + 1*x2_i + sqrt(nu_i)*u_i\n",
    "    Diseno 0: u~N(0,1), nu=1\n",
    "    Diseno 1: u~N(0,1), nu=exp(0.25*x1 + 0.25*x2)\n",
    "    Diseno 2: u~t5,     nu=exp(0.25*x1 + 0.25*x2)\n",
    "    \"\"\"\n",
    "    n = len(x1)\n",
    "\n",
    "    if design == 0:\n",
    "        u = rng.normal(0, 1, size=n)\n",
    "        nu = np.ones(n)\n",
    "    elif design == 1:\n",
    "        u = rng.normal(0, 1, size=n)\n",
    "        nu = np.exp(0.25 * x1 + 0.25 * x2)\n",
    "    elif design == 2:\n",
    "        u = rng.standard_t(df=5, size=n)\n",
    "        nu = np.exp(0.25 * x1 + 0.25 * x2)\n",
    "\n",
    "    y = 1 + 1 * x1 + 1 * x2 + np.sqrt(nu) * u\n",
    "    return y\n",
    "\n",
    "\n",
    "def white_test(y, X):\n",
    "    \"\"\"\n",
    "    Test de White para heterocedasticidad.\n",
    "    1. OLS del modelo original, obtener residuos\n",
    "    2. Regresion auxiliar: e^2 ~ 1 + x1 + x2 + x1^2 + x2^2 + x1*x2\n",
    "    3. Estadistico W = n * R^2 ~ chi2(q)\n",
    "    Retorna el p-valor del test.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # Paso 1: OLS y residuos\n",
    "    beta_hat = ols(y, X)\n",
    "    e = y - X @ beta_hat\n",
    "    e2 = e ** 2\n",
    "\n",
    "    # Paso 2: Regresion auxiliar\n",
    "    # X tiene columnas: [1, x1, x2]\n",
    "    x1 = X[:, 1]\n",
    "    x2 = X[:, 2]\n",
    "    Z = np.column_stack([\n",
    "        np.ones(n),   # constante\n",
    "        x1,           # x1\n",
    "        x2,           # x2\n",
    "        x1 ** 2,      # x1^2\n",
    "        x2 ** 2,      # x2^2\n",
    "        x1 * x2       # x1*x2\n",
    "    ])\n",
    "\n",
    "    # OLS de e^2 sobre Z\n",
    "    alpha_hat = ols(e2, Z)\n",
    "    e2_fitted = Z @ alpha_hat\n",
    "    ss_res = np.sum((e2 - e2_fitted) ** 2)\n",
    "    ss_tot = np.sum((e2 - np.mean(e2)) ** 2)\n",
    "    R2 = 1 - ss_res / ss_tot\n",
    "\n",
    "    # Paso 3: Estadistico W = n * R^2\n",
    "    q = Z.shape[1] - 1  # grados de libertad (regresores sin constante = 5)\n",
    "    W = n * R2\n",
    "    p_value = 1 - stats.chi2.cdf(W, df=q)\n",
    "\n",
    "    return p_value\n",
    "\n",
    "\n",
    "def run_white_test_simulation(n, designs, rng_seed=SEED):\n",
    "    \"\"\"\n",
    "    Ejecuta 5000 simulaciones del test de White para los disenos dados.\n",
    "    Reporta tamano (diseno 0) y poder (disenos 1 y 2).\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for design in designs:\n",
    "        rng = np.random.RandomState(rng_seed)\n",
    "        # Generar x1, x2 una sola vez (son fijos across simulations)\n",
    "        rng_x = np.random.RandomState(rng_seed)\n",
    "        x1, x2 = generate_x_part2(n, rng_x)\n",
    "        X = np.column_stack([np.ones(n), x1, x2])\n",
    "\n",
    "        p_values = np.zeros(N_SIM)\n",
    "        for sim in range(N_SIM):\n",
    "            y = generate_y_part2(x1, x2, design, rng)\n",
    "            p_values[sim] = white_test(y, X)\n",
    "\n",
    "        # Tasa de rechazo a distintos niveles\n",
    "        reject_1 = np.mean(p_values < 0.01) * 100\n",
    "        reject_5 = np.mean(p_values < 0.05) * 100\n",
    "        reject_10 = np.mean(p_values < 0.10) * 100\n",
    "\n",
    "        results[design] = {\n",
    "            'reject_1': reject_1,\n",
    "            'reject_5': reject_5,\n",
    "            'reject_10': reject_10,\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_white_results(results, n):\n",
    "    \"\"\"Imprime resultados del test de White.\"\"\"\n",
    "    print(\"=\" * 65)\n",
    "    print(f\"  Test de White - n = {n}\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    if 0 in results:\n",
    "        r = results[0]\n",
    "        print(f\"\\n  Diseno 0 (homocedasticidad) - TAMANO del test:\")\n",
    "        print(f\"    Al  1%: {r['reject_1']:.2f}%  (nominal:  1%)\")\n",
    "        print(f\"    Al  5%: {r['reject_5']:.2f}%  (nominal:  5%)\")\n",
    "        print(f\"    Al 10%: {r['reject_10']:.2f}%  (nominal: 10%)\")\n",
    "\n",
    "    if 1 in results:\n",
    "        r = results[1]\n",
    "        print(f\"\\n  Diseno 1 (normal + heterocedasticidad) - PODER del test:\")\n",
    "        print(f\"    Al  1%: {r['reject_1']:.2f}%\")\n",
    "        print(f\"    Al  5%: {r['reject_5']:.2f}%\")\n",
    "        print(f\"    Al 10%: {r['reject_10']:.2f}%\")\n",
    "\n",
    "    if 2 in results:\n",
    "        r = results[2]\n",
    "        print(f\"\\n  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\")\n",
    "        print(f\"    Al  1%: {r['reject_1']:.2f}%\")\n",
    "        print(f\"    Al  5%: {r['reject_5']:.2f}%\")\n",
    "        print(f\"    Al 10%: {r['reject_10']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1a) Diseno 0 (tamano) + 2.1b) Disenos 1 y 2 (poder) para todos los n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "  Test de White - n = 20\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 0.48%  (nominal:  1%)\n",
      "    Al  5%: 4.60%  (nominal:  5%)\n",
      "    Al 10%: 9.04%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 0.60%\n",
      "    Al  5%: 6.00%\n",
      "    Al 10%: 11.38%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 0.76%\n",
      "    Al  5%: 6.92%\n",
      "    Al 10%: 12.26%\n",
      "\n",
      "=================================================================\n",
      "  Test de White - n = 60\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 0.90%  (nominal:  1%)\n",
      "    Al  5%: 4.86%  (nominal:  5%)\n",
      "    Al 10%: 9.94%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 5.14%\n",
      "    Al  5%: 14.66%\n",
      "    Al 10%: 23.86%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 3.44%\n",
      "    Al  5%: 11.20%\n",
      "    Al 10%: 18.06%\n",
      "\n",
      "=================================================================\n",
      "  Test de White - n = 100\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 1.08%  (nominal:  1%)\n",
      "    Al  5%: 4.70%  (nominal:  5%)\n",
      "    Al 10%: 9.78%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 11.04%\n",
      "    Al  5%: 26.66%\n",
      "    Al 10%: 39.46%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 5.06%\n",
      "    Al  5%: 15.10%\n",
      "    Al 10%: 22.88%\n",
      "\n",
      "=================================================================\n",
      "  Test de White - n = 200\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 0.96%  (nominal:  1%)\n",
      "    Al  5%: 4.88%  (nominal:  5%)\n",
      "    Al 10%: 9.42%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 33.44%\n",
      "    Al  5%: 58.92%\n",
      "    Al 10%: 72.06%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 11.24%\n",
      "    Al  5%: 26.82%\n",
      "    Al 10%: 38.78%\n",
      "\n",
      "=================================================================\n",
      "  Test de White - n = 400\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 1.02%  (nominal:  1%)\n",
      "    Al  5%: 5.34%  (nominal:  5%)\n",
      "    Al 10%: 9.52%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 79.92%\n",
      "    Al  5%: 93.86%\n",
      "    Al 10%: 97.26%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 28.48%\n",
      "    Al  5%: 52.40%\n",
      "    Al 10%: 64.68%\n",
      "\n",
      "=================================================================\n",
      "  Test de White - n = 600\n",
      "=================================================================\n",
      "\n",
      "  Diseno 0 (homocedasticidad) - TAMANO del test:\n",
      "    Al  1%: 1.24%  (nominal:  1%)\n",
      "    Al  5%: 4.90%  (nominal:  5%)\n",
      "    Al 10%: 10.28%  (nominal: 10%)\n",
      "\n",
      "  Diseno 1 (normal + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 97.52%\n",
      "    Al  5%: 99.62%\n",
      "    Al 10%: 99.86%\n",
      "\n",
      "  Diseno 2 (t5 + heterocedasticidad) - PODER del test:\n",
      "    Al  1%: 47.88%\n",
      "    Al  5%: 71.14%\n",
      "    Al 10%: 80.78%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Correr test de White para todos los tamanos de muestra y los 3 disenos\n",
    "sample_sizes_p2 = [20, 60, 100, 200, 400, 600]\n",
    "\n",
    "for n in sample_sizes_p2:\n",
    "    res = run_white_test_simulation(n, designs=[0, 1, 2])\n",
    "    print_white_results(res, n)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Correccion de la matriz de varianzas y covarianzas (White)\n",
    "\n",
    "Sesgo relativo: $b_j = \\frac{1}{S}\\sum_{s=1}^{S} \\frac{\\widehat{Var}(\\hat\\beta_j)^{(s)} - Var(\\hat\\beta_j)}{Var(\\hat\\beta_j)}$\n",
    "\n",
    "Sesgo relativo total: $|b_0| + |b_1| + |b_2|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  Sesgo relativo de White (residuos OLS)\n",
      "======================================================================\n",
      "\n",
      "--- Diseno 1 ---\n",
      "      n          b0          b1          b2    |b0|+|b1|+|b2|\n",
      "     20     -0.1592     -0.2099     -0.2123            0.5813\n",
      "     60     -0.0511     -0.0677     -0.0644            0.1831\n",
      "    100     -0.0317     -0.0430     -0.0407            0.1154\n",
      "    200     -0.0160     -0.0204     -0.0204            0.0568\n",
      "    400     -0.0070     -0.0093     -0.0088            0.0251\n",
      "    600     -0.0048     -0.0055     -0.0067            0.0171\n",
      "\n",
      "--- Diseno 2 ---\n",
      "      n          b0          b1          b2    |b0|+|b1|+|b2|\n",
      "     20     -0.1546     -0.2071     -0.2226            0.5843\n",
      "     60     -0.0614     -0.0730     -0.0790            0.2133\n",
      "    100     -0.0364     -0.0443     -0.0466            0.1273\n",
      "    200     -0.0182     -0.0204     -0.0250            0.0636\n",
      "    400     -0.0094     -0.0093     -0.0140            0.0327\n",
      "    600     -0.0049     -0.0036     -0.0088            0.0173\n"
     ]
    }
   ],
   "source": [
    "def white_variance_simulation(n, design, use_true_errors=False, rng_seed=SEED):\n",
    "    \"\"\"\n",
    "    Parte 2.2: Sesgo relativo de la estimacion de White de la matriz de var-cov.\n",
    "    \n",
    "    Var_true = (X'X)^{-1} X' Omega_true X (X'X)^{-1}  (varianza verdadera de OLS)\n",
    "    Var_hat  = (X'X)^{-1} X' Omega_hat  X (X'X)^{-1}  (estimacion de White)\n",
    "    \n",
    "    Omega_hat = diag(e_hat^2) si use_true_errors=False (residuos)\n",
    "    Omega_hat = diag(epsilon^2) si use_true_errors=True (errores verdaderos)\n",
    "    \"\"\"\n",
    "    rng_x = np.random.RandomState(rng_seed)\n",
    "    x1, x2 = generate_x_part2(n, rng_x)\n",
    "    X = np.column_stack([np.ones(n), x1, x2])\n",
    "    \n",
    "    # Varianza de u segun diseno\n",
    "    if design == 1:\n",
    "        var_u = 1.0  # u ~ N(0,1)\n",
    "    elif design == 2:\n",
    "        var_u = 5.0 / 3.0  # u ~ t5, Var(t5) = 5/(5-2)\n",
    "    \n",
    "    # nu_i para heterocedasticidad\n",
    "    nu = np.exp(0.25 * x1 + 0.25 * x2)\n",
    "    \n",
    "    # Varianza verdadera de epsilon_i = sqrt(nu_i)*u_i es: nu_i * Var(u)\n",
    "    sigma2_true = nu * var_u\n",
    "    Omega_true = np.diag(sigma2_true)\n",
    "    \n",
    "    # Varianza verdadera de beta_hat OLS: (X'X)^{-1} X' Omega_true X (X'X)^{-1}\n",
    "    XtX_inv = np.linalg.inv(X.T @ X)\n",
    "    Var_true = XtX_inv @ X.T @ Omega_true @ X @ XtX_inv\n",
    "    var_true_diag = np.diag(Var_true)  # [Var(b0), Var(b1), Var(b2)]\n",
    "    \n",
    "    # Simulacion\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    relative_bias = np.zeros((N_SIM, 3))  # para b0, b1, b2\n",
    "    \n",
    "    for sim in range(N_SIM):\n",
    "        # Generar errores\n",
    "        if design == 1:\n",
    "            u = rng.normal(0, 1, size=n)\n",
    "        elif design == 2:\n",
    "            u = rng.standard_t(df=5, size=n)\n",
    "        \n",
    "        epsilon = np.sqrt(nu) * u\n",
    "        y = 1 + 1 * x1 + 1 * x2 + epsilon\n",
    "        \n",
    "        # OLS\n",
    "        beta_hat = ols(y, X)\n",
    "        e_hat = y - X @ beta_hat\n",
    "        \n",
    "        # Estimacion de White\n",
    "        if use_true_errors:\n",
    "            Omega_hat = np.diag(epsilon ** 2)  # errores verdaderos\n",
    "        else:\n",
    "            Omega_hat = np.diag(e_hat ** 2)    # residuos OLS\n",
    "        \n",
    "        Var_hat = XtX_inv @ X.T @ Omega_hat @ X @ XtX_inv\n",
    "        var_hat_diag = np.diag(Var_hat)\n",
    "        \n",
    "        # Sesgo relativo para esta simulacion\n",
    "        relative_bias[sim, :] = (var_hat_diag - var_true_diag) / var_true_diag\n",
    "    \n",
    "    # Promediar sobre simulaciones\n",
    "    b = np.mean(relative_bias, axis=0)  # b0, b1, b2\n",
    "    total_bias = np.sum(np.abs(b))\n",
    "    \n",
    "    return b, total_bias\n",
    "\n",
    "\n",
    "def run_white_variance_all(use_true_errors=False):\n",
    "    \"\"\"Corre la simulacion 2.2 para todos los n y disenos.\"\"\"\n",
    "    sample_sizes = [20, 60, 100, 200, 400, 600]\n",
    "    label = \"errores verdaderos\" if use_true_errors else \"residuos OLS\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Sesgo relativo de White ({label})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for design in [1, 2]:\n",
    "        print(f\"\\n--- Diseno {design} ---\")\n",
    "        print(f\"  {'n':>5s}  {'b0':>10s}  {'b1':>10s}  {'b2':>10s}  {'|b0|+|b1|+|b2|':>16s}\")\n",
    "        \n",
    "        for n in sample_sizes:\n",
    "            b, total = white_variance_simulation(n, design, use_true_errors)\n",
    "            print(f\"  {n:5d}  {b[0]:10.4f}  {b[1]:10.4f}  {b[2]:10.4f}  {total:16.4f}\")\n",
    "\n",
    "\n",
    "# a-f) Con residuos OLS (estimacion estandar de White)\n",
    "run_white_variance_all(use_true_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2g) Repetir con errores verdaderos en vez de residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  Sesgo relativo de White (errores verdaderos)\n",
      "======================================================================\n",
      "\n",
      "--- Diseno 1 ---\n",
      "      n          b0          b1          b2    |b0|+|b1|+|b2|\n",
      "     20      0.0018     -0.0008      0.0144            0.0169\n",
      "     60      0.0006      0.0011      0.0069            0.0086\n",
      "    100     -0.0006     -0.0015      0.0031            0.0052\n",
      "    200     -0.0003      0.0004      0.0014            0.0021\n",
      "    400      0.0008      0.0008      0.0019            0.0034\n",
      "    600      0.0004      0.0013      0.0005            0.0023\n",
      "\n",
      "--- Diseno 2 ---\n",
      "      n          b0          b1          b2    |b0|+|b1|+|b2|\n",
      "     20     -0.0018     -0.0109     -0.0158            0.0286\n",
      "     60     -0.0097     -0.0067     -0.0086            0.0250\n",
      "    100     -0.0050     -0.0039     -0.0036            0.0125\n",
      "    200     -0.0025     -0.0006     -0.0042            0.0073\n",
      "    400     -0.0017      0.0005     -0.0035            0.0057\n",
      "    600      0.0003      0.0030     -0.0019            0.0052\n"
     ]
    }
   ],
   "source": [
    "# g) Con errores verdaderos (epsilon^2 en vez de e_hat^2)\n",
    "run_white_variance_all(use_true_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}